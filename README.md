# Multi-Armed Bandit Algorithms: E-Greedy, UCB, and Thompson Sampling

This project contains a Jupyter Notebook that implements and analyzes three popular algorithms for solving the **multi-armed bandit (MAB)** problem:

- **Epsilon-Greedy**
- **Upper Confidence Bound (UCB)**
- **Thompson Sampling**
